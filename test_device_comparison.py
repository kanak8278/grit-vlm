"""
Compare CPU vs MPS vs CUDA performance for GRIT-VLM.
"""

import torch
import time
from transformers import Idefics3ForConditionalGeneration
from PIL import Image
import numpy as np
import warnings
warnings.filterwarnings("ignore")

from grit_vlm import GRITLoRAConfig
from grit_vlm.models.vlm_adapter import VLMGRITAdapter
from grit_vlm.config import (
    DeviceStrategy, get_available_devices, get_device_info,
    optimize_model_kwargs, print_device_summary, get_device_config
)

def test_device_performance():
    """Test GRIT performance across all available devices."""
    print("üñ•Ô∏è  Device Performance Comparison")
    print("=" * 40)
    
    # Show device info
    device_info = get_device_info()
    available = device_info["available_devices"]
    
    print(f"üìã Available devices:")
    for device, avail in available.items():
        status = "‚úÖ" if avail else "‚ùå"
        print(f"  {status} {device.upper()}")
        if device == "cuda" and avail:
            print(f"      ‚îî‚îÄ {device_info['cuda_device_name']}")
    
    # Test configurations
    configs = []
    
    # Always test CPU
    configs.append({"name": "CPU", "device_map": "cpu", "torch_dtype": torch.float32})
    
    # Test MPS if available
    if available["mps"]:
        configs.append({"name": "MPS", "device_map": "auto", "torch_dtype": torch.float16, "force_mps": True})
    
    # Test CUDA if available 
    if available["cuda"]:
        configs.append({"name": "CUDA", "device_map": "auto", "torch_dtype": torch.float16, "force_cuda": True})
    
    for config in configs:
        print(f"\nüß™ Testing {config['name']} Configuration")
        print("-" * 30)
        
        try:
            # Load model with device-specific config
            start_time = time.time()
            
            # Force specific device if needed
            if config.get("force_mps") and not available["cuda"]:
                # Ensure MPS is used when CUDA not available
                pass
            elif config.get("force_cuda") and available["cuda"]:
                # Ensure CUDA is used
                pass
            
            model = Idefics3ForConditionalGeneration.from_pretrained(
                "HuggingFaceTB/SmolVLM-256M-Instruct",
                torch_dtype=config["torch_dtype"],
                device_map=config["device_map"],
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            load_time = time.time() - start_time
            
            # Get actual device
            device = next(model.parameters()).device
            dtype = next(model.parameters()).dtype
            print(f"‚úì Model loaded on {device} ({dtype}) in {load_time:.2f}s")
            
            # Apply GRIT
            start_time = time.time()
            grit_config = GRITLoRAConfig(r=4, lora_alpha=8)
            adapter = VLMGRITAdapter(
                model=model,
                config=grit_config,
                model_config_name="smolvlm_fast"
            )
            grit_time = time.time() - start_time
            print(f"‚úì GRIT applied in {grit_time:.2f}s")
            
            # Test inference
            test_image = Image.fromarray((np.ones((64, 64, 3)) * [255, 0, 0]).astype(np.uint8))
            
            # Prepare simple input (skip processor for speed)
            input_ids = torch.randint(0, 1000, (1, 10)).to(device)
            
            # Warm up
            with torch.no_grad():
                _ = model(input_ids=input_ids)
            
            # Time inference
            start_time = time.time()
            num_runs = 5
            for _ in range(num_runs):
                with torch.no_grad():
                    outputs = model(input_ids=input_ids)
            inference_time = (time.time() - start_time) / num_runs
            
            print(f"‚úì Average inference time: {inference_time*1000:.2f}ms")
            
            # Show memory usage
            if device.type == "cuda":
                memory_mb = torch.cuda.memory_allocated() / 1024**2
                print(f"‚úì CUDA memory: {memory_mb:.1f} MB")
            else:
                print(f"‚úì Memory: Not tracked for {device.type}")
            
            # Test backward pass
            start_time = time.time()
            outputs = model(input_ids=input_ids)
            loss = outputs.logits.sum()
            loss.backward()
            backward_time = time.time() - start_time
            print(f"‚úì Backward pass time: {backward_time*1000:.2f}ms")
            
            # Cleanup
            del model, adapter
            
            # Clear device cache
            if device.type == "cuda":
                torch.cuda.empty_cache()
            elif device.type == "mps":
                torch.mps.empty_cache()
            
        except Exception as e:
            print(f"‚ùå Failed: {e}")

def test_device_strategies():
    """Test different device selection strategies."""
    print("\nüéØ Device Strategy Testing")
    print("=" * 30)
    
    strategies = [
        ("auto_performance", "Best performance"),
        ("auto_inference", "Optimized for inference"),
        ("stable_training", "Most stable training"),
        ("memory_efficient", "Memory efficient")
    ]
    
    for config_name, description in strategies:
        print(f"\nüîß {config_name}: {description}")
        try:
            kwargs = get_device_config(config_name)
            device_map = kwargs["device_map"]
            dtype = kwargs["torch_dtype"]
            print(f"  ‚îî‚îÄ Device: {device_map}, Type: {dtype}")
        except Exception as e:
            print(f"  ‚îî‚îÄ Error: {e}")

def show_device_recommendations():
    """Show device recommendations based on available hardware."""
    print("\nüí° Device Recommendations")
    print("=" * 30)
    
    available = get_available_devices()
    
    print("üìã For different use cases:")
    
    if available["cuda"]:
        print("üöÄ CUDA Available:")
        print("  ‚Ä¢ Training: Use CUDA (fastest backward passes)")  
        print("  ‚Ä¢ Inference: Use CUDA (fastest overall)")
        print("  ‚Ä¢ Memory: Use CUDA with max_memory limits")
        
    elif available["mps"]:
        print("üçé MPS Available (Apple Silicon):")
        print("  ‚Ä¢ Training: Use CPU (more stable backward passes)")
        print("  ‚Ä¢ Inference: Use MPS (1.7x faster than CPU)")
        print("  ‚Ä¢ Memory: Use CPU (more predictable)")
        
    else:
        print("üíª CPU Only:")
        print("  ‚Ä¢ Training: CPU with float32")
        print("  ‚Ä¢ Inference: CPU (only option)")
        print("  ‚Ä¢ Memory: Use low_cpu_mem_usage=True")
    
    print(f"\nüîß Quick configs:")
    print(f"  ‚Ä¢ Fast training: get_device_config('stable_training')")
    print(f"  ‚Ä¢ Fast inference: get_device_config('auto_inference')")
    print(f"  ‚Ä¢ Memory efficient: get_device_config('memory_efficient')")

if __name__ == "__main__":
    print("üöÄ GRIT-VLM Device Configuration Test")
    print("=" * 55)
    
    # Test device performance
    test_device_performance()
    
    # Test device strategies
    test_device_strategies()
    
    # Show recommendations
    show_device_recommendations()